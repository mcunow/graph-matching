{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gnn\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import utils\n",
    "import util_metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gnn_decoder\n",
    "from torch_geometric.data import Data,Batch\n",
    "from torch_geometric.utils import to_dense_adj,to_dense_batch,get_embeddings\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import vgae\n",
    "import copy\n",
    "import visualize\n",
    "import wandb\n",
    "import utils\n",
    "import matching\n",
    "from decoder import Decoder\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path='../data/QM9_canonical.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_smiles = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/dataset_small.pkl.gz', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims=100\n",
    "edge_dim=4\n",
    "node_dim=4\n",
    "train_loader=DataLoader(dataset,4,shuffle=True)\n",
    "matcher=matching.NaiveMatcher()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder=gnn.GINE(in_channels=node_dim,hidden_channels=[16,32,64,128],out_channels=128,norm=\"batch\",num_layers=4,\n",
    "                  act=\"leakyrelu\",edge_dim=4,dropout=0.1).to(device)\n",
    "#decoder=gnn_decoder.Generator_Decoder(latent_dim=latent_dims,node_dim=node_dim,edge_dim=edge_dim,num_layers=4).to(device)\n",
    "decoder=Decoder(latent_dims)\n",
    "matcher=matching.NaiveMatcher()\n",
    "\n",
    "vgae_model=vgae.VGAE(encoder=encoder,decoder=decoder,latent_dims=latent_dims,embedding=\"graph\",eval=False).to(device)\n",
    "#vgae_model.set_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [01:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m kl_loss\u001b[38;5;241m=\u001b[39mbeta\u001b[38;5;241m*\u001b[39mvgae_model\u001b[38;5;241m.\u001b[39mkl_loss(mu,logvar)\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m=\u001b[39mmatched_loss\u001b[38;5;241m+\u001b[39mkl_loss\u001b[38;5;241m+\u001b[39mnum_loss\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#Log data and evaluate reconstruction quality\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#valid_rec,num_rec=util_metrics.evaluate_reconstruction(data,data_rec)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgae_model.train()\n",
    "optimizer = torch.optim.Adam(vgae_model.parameters(), lr=1e-5,weight_decay=1e-5)\n",
    "kl_loss_ls,matched_loss_ls,unmatched_loss_ls,num_loss_ls=[],[],[],[]\n",
    "alpha=0.1\n",
    "beta=0.1\n",
    "\n",
    "for epoch in tqdm(range(2)):\n",
    "    kl_ls=[]\n",
    "    matched_ls=[]\n",
    "    unmatched_ls=[]\n",
    "    num_ls=[]\n",
    "    #vgae_model.set_encoder()\n",
    "    for data in train_loader:\n",
    "      data = data.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      data_rec,mu,logvar,_,pred_num= vgae_model(data)\n",
    "\n",
    "      num_nodes=torch.bincount(data.batch).to(torch.float32)\n",
    "      num_loss=alpha*F.mse_loss(pred_num,num_nodes)\n",
    "\n",
    "      matched_loss,unmatched_loss=matcher.diffusion_matching(data,data_rec,test=False)\n",
    "      \n",
    "     \n",
    "      kl_loss=beta*vgae_model.kl_loss(mu,logvar)\n",
    "      loss=matched_loss+kl_loss+num_loss\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      #Log data and evaluate reconstruction quality\n",
    "      #valid_rec,num_rec=util_metrics.evaluate_reconstruction(data,data_rec)\n",
    "\n",
    "      optimizer.step()\n",
    "      kl_ls.append(kl_loss.item())\n",
    "      matched_ls.append(matched_loss.item())\n",
    "      unmatched_ls.append(unmatched_loss.item())\n",
    "      num_ls.append(num_loss.item())\n",
    "    kl_loss_ls.append(np.array(kl_ls).mean())\n",
    "    matched_loss_ls.append(np.array(matched_ls).mean())\n",
    "    unmatched_loss_ls.append(np.array(unmatched_ls).mean())\n",
    "    num_loss_ls.append(np.array(num_ls).mean())\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor(1).unsqueeze(0),torch.tensor([2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (1132239008.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    PNA(in_channels=16+9,hidden_channels=32,num_layers=3,out_channels=32,dropout=0.,act=\"leakyrelu\",aggregators=\"sum\",scalers=)\u001b[0m\n\u001b[0m                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.models import GIN,PNA\n",
    "from torch_geometric.utils  import add_self_loops\n",
    "\n",
    "PNA(in_channels=16+9,hidden_channels=32,num_layers=3,out_channels=32,dropout=0.,act=\"leakyrelu\",aggregators=[\"sum\",\"mean\",\"max\"],scalers=\"linear\",\n",
    "    deg=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = max(torch.max(data.edge_index) for data in dataset) + 1\n",
    "total_degrees = torch.zeros(num_nodes, dtype=torch.int32)\n",
    "for data in dataset:\n",
    "    degrees = torch.zeros(num_nodes, dtype=torch.int32)\n",
    "    for edge in zip(data.edge_index[0],data.edge_index[1]):\n",
    "        u, v = edge\n",
    "        degrees[u] += 1\n",
    "        degrees[v] += 1  # Undirected graph means both nodes share the edge\n",
    "    total_degrees += degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import  degree\n",
    "DEVICE=\"cpu\"\n",
    "def dataset_to_degree_bin(train_dataset):\n",
    "  \"\"\"\n",
    "  Convert a dataset to a histogram of node degrees (in-degrees).\n",
    "  Load from file if available; otherwise, compute from the dataset.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "  # Compute the maximum in-degree in the training data.\n",
    "  max_degree = -1\n",
    "  for data in train_dataset:\n",
    "    data = data\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    max_degree = max(max_degree, int(d.max()))\n",
    "\n",
    "  # Create an empty histogram for degrees.\n",
    "  deg = torch.zeros(max_degree + 1, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "  # Populate the histogram with data from the dataset.\n",
    "  for data in train_dataset:\n",
    "    data = data.to(DEVICE)\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "\n",
    "  return deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4223, 7698, 4767,  927])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_degree_bin(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4464],\n",
       "         [0.8829],\n",
       "         [0.1657],\n",
       "         [0.8372],\n",
       "         [0.3358],\n",
       "         [0.5789]],\n",
       "\n",
       "        [[0.3690],\n",
       "         [0.9846],\n",
       "         [0.9116],\n",
       "         [0.2970],\n",
       "         [0.2022],\n",
       "         [0.7362]],\n",
       "\n",
       "        [[0.1399],\n",
       "         [0.6301],\n",
       "         [0.7738],\n",
       "         [0.6097],\n",
       "         [0.9382],\n",
       "         [0.8478]],\n",
       "\n",
       "        [[0.3273],\n",
       "         [0.4601],\n",
       "         [0.5417],\n",
       "         [0.1121],\n",
       "         [0.2040],\n",
       "         [0.1708]],\n",
       "\n",
       "        [[0.9455],\n",
       "         [0.4811],\n",
       "         [0.9671],\n",
       "         [0.3204],\n",
       "         [0.8872],\n",
       "         [0.4423]],\n",
       "\n",
       "        [[0.1172],\n",
       "         [0.7680],\n",
       "         [0.9519],\n",
       "         [0.3635],\n",
       "         [0.9462],\n",
       "         [0.6773]],\n",
       "\n",
       "        [[0.5267],\n",
       "         [0.0653],\n",
       "         [0.1442],\n",
       "         [0.7328],\n",
       "         [0.4561],\n",
       "         [0.6171]],\n",
       "\n",
       "        [[0.2665],\n",
       "         [0.8870],\n",
       "         [0.4843],\n",
       "         [0.1504],\n",
       "         [0.6513],\n",
       "         [0.0931]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((8,6)).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor of shape (n, 9, 9, 4)\n",
    "n = 3  # Example batch size\n",
    "tensor = torch.rand((9, 9, 4))\n",
    "\n",
    "# Create a new tensor with an additional 0 at the start of the fourth dimension\n",
    "zeros = torch.zeros(( 9, 9, 1))\n",
    "expanded_tensor = torch.cat((zeros, tensor), dim=2)\n",
    "\n",
    "print(expanded_tensor.shape)  # Should be (n, 9, 9, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae_model.eval()\n",
    "sample=vgae_model.sample(wandb.config[\"sample_size\"])    \n",
    "validity,unique,novelty,_=util_metrics.compute_metrics(sample,dataset_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"YOUR/PATH/HERE\"\n",
    "torch.save(vgae_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Samples. Compute fraction of valid, unique, and novel molecules. Return SMILES representation of all novel SMILES.\n",
    "Plot generated molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae_model.eval()\n",
    "sample=vgae_model.sample(1000)\n",
    "validity,unique,novel,smiles_list=util_metrics.compute_metrics(sample,dataset_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "mol_list = [Chem.MolFromSmiles(smiles) for smiles in list(smiles_list)]\n",
    "Draw.MolsToGridImage(mol_list[:50], molsPerRow=10, subImgSize=(400,400),maxMols=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
